name: Daily ebooks Scraping Job

on:
  schedule:
    - cron: '0 2 * * *'
  workflow_dispatch:

jobs:
  run-scraping:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip gnupg
        
        # Install Chrome
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome-keyring.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        google-chrome --version
        
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        
        # Main dependencies
        pip install selenium pandas webdriver-manager lxml
        
        # Google Drive API dependencies
        pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
        
        # If requirements.txt exists, install it too
        if [ -f "requirements.txt" ]; then
          pip install -r requirements.txt
        fi
        
    - name: Run Python scraping script
      env:
        # Environment variables for Chrome
        DISPLAY: :99
        PYTHONUNBUFFERED: 1
        
        # Google Drive credentials (configure this in GitHub Secrets)
        GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
      run: |
        echo "üìÅ Working directory: $(pwd)"
        echo "üìÅ Repository structure:"
        ls -la
        echo ""
        echo "üìÅ Contents of scrips_py directory:"
        ls -la scrips_py/ 2>/dev/null || echo "scrips_py directory not found or empty"
        
        echo "üöÄ Starting MediaMarkt ebooks scraping..."
        echo "üîÑ Configuring Google Drive..."
        
        # Check if Google Drive credentials are available
        if [ -z "$GOOGLE_CREDENTIALS_JSON" ]; then
          echo "‚ö†Ô∏è  WARNING: No Google Drive credentials configured"
          echo "‚ÑπÔ∏è   The script will run but won't update Google Drive"
        else:
          echo "‚úÖ Google Drive credentials configured"
        fi
        
        # Create directory for results
        mkdir -p scraping_results
        
        # Check for the script in scrips_py directory
        if [ -f "scrips_py/01_scrip_ebooks.py" ]; then
          echo "‚úÖ Found script at: scrips_py/01_scrip_ebooks.py"
          SCRIPT_PATH="scrips_py/01_scrip_ebooks.py"
        elif [ -f "scrips_py/scrip_ebooks.py" ]; then
          echo "‚úÖ Found script at: scrips_py/scrip_ebooks.py"
          SCRIPT_PATH="scrips_py/scrip_ebooks.py"
        else
          echo "‚ùå Script not found in scrips_py directory."
          echo "üîç Looking for Python files in scrips_py..."
          find scrips_py -name "*.py" 2>/dev/null || echo "No Python files found in scrips_py"
          echo ""
          echo "üîç Looking for script files in entire repository..."
          find . -name "*ebook*.py" -type f 2>/dev/null || echo "No ebook scraping scripts found"
          exit 1
        fi
        
        # Execute the script
        echo "üöÄ Executing script: $SCRIPT_PATH"
        python $SCRIPT_PATH
        
        echo "‚úÖ Script executed. Checking results..."
        
        # List generated files
        echo "üìÑ Generated files in scraping_results/:"
        ls -la scraping_results/ 2>/dev/null || echo "No results in scraping_results/"
        echo ""
        echo "üìÑ All CSV files in repository:"
        find . -name "*.csv" -type f 2>/dev/null | head -20 || echo "No CSV files found"
        
    - name: Upload results as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: scraping-output
        path: |
          scraping_results/*.csv
          *.csv
        retention-days: 7
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå MediaMarkt scraping has failed"
        echo "Check the logs for more details"
        echo ""
        echo "Possible issues:"
        echo "1. Script not found in scrips_py directory"
        echo "2. Google Drive credentials missing or incorrect"
        echo "3. Website structure changed"
        echo "4. Chrome installation failed"
