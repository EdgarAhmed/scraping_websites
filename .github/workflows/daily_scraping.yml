name: Daily Scraping Job

on:
  schedule:
    - cron: '0 8 * * *'
  workflow_dispatch:

jobs:
  run-scraping:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'
        
    - name: Install Chrome
      run: |
        sudo apt-get update
        sudo apt-get install -y wget unzip gnupg
        
        # Install Chrome
        wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo gpg --dearmor -o /usr/share/keyrings/google-chrome-keyring.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/google-chrome-keyring.gpg] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        google-chrome --version
        
    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        
        # Main dependencies
        pip install selenium pandas webdriver-manager lxml
        
        # Google Drive API dependencies
        pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib
        
        # If requirements.txt exists, install it too
        if [ -f "requirements.txt" ]; then
          pip install -r requirements.txt
        fi
        
    - name: Run Python scraping script
      env:
        # Environment variables for Chrome
        DISPLAY: :99
        PYTHONUNBUFFERED: 1
        
        # Google Drive credentials (configure this in GitHub Secrets)
        GOOGLE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_CREDENTIALS_JSON }}
      run: |
        echo "üìÅ Working directory: $(pwd)"
        echo "üìÅ Repository content:"
        ls -la
        
        echo "üöÄ Starting MediaMarkt ebooks scraping..."
        echo "üîÑ Configuring Google Drive..."
        
        # Check if Google Drive credentials are available
        if [ -z "$GOOGLE_CREDENTIALS_JSON" ]; then
          echo "‚ö†Ô∏è  WARNING: No Google Drive credentials configured"
          echo "‚ÑπÔ∏è   The script will run but won't update Google Drive"
        else
          echo "‚úÖ Google Drive credentials configured"
        fi
        
        # Create directory for results
        mkdir -p scraping_results
        
        # Find and execute the Python script
        # Check common locations for the script
        if [ -f "scrip_ebooks_01.py" ]; then
          echo "üìÑ Found script in root: scrip_ebooks_01.py"
          SCRIPT_PATH="scrip_ebooks_01.py"
        elif [ -f "scraping_scripts/scrip_ebooks_01.py" ]; then
          echo "üìÑ Found script in scraping_scripts folder"
          SCRIPT_PATH="scraping_scripts/scrip_ebooks_01.py"
        elif [ -f "scripts/scrip_ebooks_01.py" ]; then
          echo "üìÑ Found script in scripts folder"
          SCRIPT_PATH="scripts/scrip_ebooks_01.py"
        elif [ -f "src/scrip_ebooks_01.py" ]; then
          echo "üìÑ Found script in src folder"
          SCRIPT_PATH="src/scrip_ebooks_01.py"
        else
          echo "‚ùå Script not found. Please check the repository structure."
          echo "üîç Looking for script files..."
          find . -name "*.py" | grep -i ebook
          exit 1
        fi
        
        # Execute the script
        python $SCRIPT_PATH
        
        echo "‚úÖ Script executed. Checking results..."
        
        # List generated files
        echo "üìÑ Generated files:"
        ls -la scraping_results/ 2>/dev/null || echo "No results in scraping_results/"
        echo ""
        echo "üìÑ All CSV files in repository:"
        find . -name "*.csv" -type f 2>/dev/null || echo "No CSV files found"
        
    - name: Upload results as artifacts
      uses: actions/upload-artifact@v4
      with:
        name: scraping-output
        path: |
          scraping_results/*.csv
          *.csv
        retention-days: 7
        
    - name: Notify on failure
      if: failure()
      run: |
        echo "‚ùå MediaMarkt scraping has failed"
        echo "Check the logs for more details"
